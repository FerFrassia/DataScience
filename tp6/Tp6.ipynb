{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Practico 6:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "import scipy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('ap.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "corpusDic = {}\n",
    "totalString = \"\"\n",
    "for doc in root:\n",
    "    docno, text = doc\n",
    "    docnum = (docno.text).replace(\" \", \"\")\n",
    "    corpusDic[docnum] = text.text\n",
    "    \n",
    "    textWithAddedSpace = text.text + \" \"\n",
    "    totalString += textWithAddedSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Levantar el corpus AP, separando cada noticia como un elemento distinto\n",
    "en un diccionario (<DOCNO> : <TEXT>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulario = {}\n",
    "stringsNoDeseados = [',', '.', \"''\", \"'\", \"!\",';',':', '``', \"__\", '@', '|', '...', '(', ')', '{', '}']\n",
    "for key in corpusDic.keys():\n",
    "    tokens = nltk.word_tokenize(corpusDic[key])\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word not in stringsNoDeseados:\n",
    "            if word in vocabulario:\n",
    "                vocabulario[word] += 1\n",
    "            else:\n",
    "                vocabulario[word] = 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "16-year-old\n",
      "student\n",
      "at\n",
      "a\n",
      "private\n",
      "Baptist\n",
      "school\n",
      "who\n",
      "allegedly\n"
     ]
    }
   ],
   "source": [
    "vtokens = nltk.word_tokenize(totalString)\n",
    "for i in range(10):\n",
    "    print(vtokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Calcular el tamaño del vocabulario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamaño del vocabulario:  47454\n"
     ]
    }
   ],
   "source": [
    "print(\"tamaño del vocabulario: \", len(vocabulario.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Para las 500 palabras con más apariciones, calcular el par más asociado según la medida presentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mostRepeated500 = [word for word in sorted(vocabulario, key=vocabulario.get, reverse=True)][0:501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures_class = nltk.collocations.BigramAssocMeasures\n",
    "\n",
    "#esto hace los bigramas de todas las palabras en el texto\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(vtokens,window_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'LL\", 'WALK'),\n",
       " (\"'Til\", 'Becomes'),\n",
       " (\"'Til\", 'Tear'),\n",
       " ('-547,000', 'sundry'),\n",
       " ('-Bgistratwissmou', 'criminalp`ummo'),\n",
       " ('-Los', 'Angeles-Tokyo'),\n",
       " ('-More', 'armor-piercing'),\n",
       " ('-Patrick', 'Kieran'),\n",
       " ('.04', '158.89'),\n",
       " ('.07', '.13'),\n",
       " ('.09', '361.08'),\n",
       " ('.11', '193.44'),\n",
       " ('.26', '195.59'),\n",
       " ('.36', '147.51'),\n",
       " ('.41', '304.54'),\n",
       " ('.43', '270.16'),\n",
       " ('.44', 'Magnums'),\n",
       " ('.51', '296.06'),\n",
       " ('.61', '66.18'),\n",
       " ('.62', '156.79'),\n",
       " ('.63', '298.34'),\n",
       " ('.64', '310.11'),\n",
       " ('.67', '75.24'),\n",
       " ('.68', '1.58'),\n",
       " ('.79', '355.54'),\n",
       " ('.89', '2,132.11'),\n",
       " ('.93', '184.04'),\n",
       " ('0.01', '155.36'),\n",
       " ('0.02', '1,878.9'),\n",
       " ('0.027', '0.109')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(finder.nbest(bigram_measures_class.pmi, 30))\n",
    "#finder.score_ngram(bigram_measures_class.pmi, '3', '/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('South', 'Africa'), 8.652361975378483),\n",
       " (('interest', 'rates'), 8.532893251786337),\n",
       " (('news', 'conference'), 8.244661043385566),\n",
       " (('New', 'York'), 8.062812652158431),\n",
       " (('United', 'States'), 8.06188036051849),\n",
       " (('West', 'German'), 7.983531966464916),\n",
       " (('East', 'Germany'), 7.762227820171429),\n",
       " (('West', 'Germany'), 7.738697068761287),\n",
       " (('Soviet', 'Union'), 7.614216942440752),\n",
       " (('told', 'reporters'), 7.557934639238603)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#para mi esto esta mal xq te hace los bigramas entre las palabras de mosrRepeated500.\n",
    "\n",
    "scores = {}\n",
    "for word_1, word_2 in itertools.combinations(mostRepeated500, 2):\n",
    "    score = finder.score_ngram(bigram_measures_class.pmi, word_1, word_2) \n",
    "    # FALTA HACER EL APPEND!! EL IF ES EL PROBLEMA!!\n",
    "    scores[(word_1, word_2)] = score if score is not None else float('-inf')\n",
    "\n",
    "aux = sorted(scores, key=scores.get, reverse=True)\n",
    "[(au, scores[au]) for au in aux[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Procesar el texto, tokenizando eliminando signos de puntuación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texto = open(\"Darwin.txt\",\"r\").read()\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "tokens = [w for w in tokens if (not w in stopset and not w in stringsNoDeseados)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2) Siguiendo el artı́culo de la sección, calcular la autocorrelación para estimar la distribución de la palabra a lo largo del texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicGausXPalabra = {}\n",
    "palabras_buscar = ['plants','instinct','for']\n",
    "for indice in range(len(tokens)):\n",
    "    if tokens[indice] in palabras_buscar:\n",
    "        if tokens[indice] in dicGausXPalabra:\n",
    "            dicGausXPalabra[tokens[indice]].append(sp.stats.norm(indice,50))\n",
    "        else:\n",
    "            dicGausXPalabra[tokens[indice]] = [sp.stats.norm(indice,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicSumGausXPalabra = {}\n",
    "for key, gausList in dicGausXPalabra.items():\n",
    "    print(key)\n",
    "    for index in range(len(tokens)):\n",
    "        sumGausianas = 0\n",
    "        for gausiana in dicGausXPalabra[key]:\n",
    "            sumGausianas += gausiana.pdf(indice)\n",
    "        dicSumGausXPalabra[key] = sumGausianas\n",
    "print(dicSumGausXPalabra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86297"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
